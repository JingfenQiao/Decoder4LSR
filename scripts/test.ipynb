{"cells":[{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from transformers import OPTForCausalLM, AutoTokenizer\n","import torch\n","from torch import nn\n","\n","model = OPTForCausalLM.from_pretrained(\"facebook/opt-350m\")\n","tokenizer = AutoTokenizer.from_pretrained(\"facebook/opt-350m\")\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model.to(device)\n","prompt = \"The meaning\"\n","kwargs = tokenizer(prompt, return_tensors='pt').to(device)\n","relu = nn.ReLU()\n","\n","def shift_right(attention_mask):\n","    batch_size, seq_length = attention_mask.size()\n","    output_mask = torch.zeros(batch_size, seq_length + 1, dtype=attention_mask.dtype)\n","    output_mask[:, :-1] = attention_mask    \n","    output_mask[:, -1] = 1\n","    return output_mask"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["N = 6\n","output_vectors = []\n","output= kwargs[\"input_ids\"]\n","\n","for _ in range(N):\n","    with torch.no_grad():\n","        outputs = model(input_ids=output, output_hidden_states=True)\n","        \n","    next_token = torch.multinomial(torch.softmax(outputs.logits[:, -1, :], dim=-1), 1)\n","    output = torch.cat([output, next_token], dim=-1)\n","    \n","    print(tokenizer.decode(output[0], skip_special_tokens=True))\n","    \n","    logits = (\n","        outputs.logits\n","        * kwargs[\"attention_mask\"].unsqueeze(-1)\n","    )\n","    logits = torch.log1p(relu(logits))\n","    \n","    print(logits)\n","    lex_weights =torch.max(logits, dim=1).values\n","    kwargs[\"attention_mask\"] = shift_right(kwargs[\"attention_mask\"])\n","    print(\"attention_mask \", kwargs[\"attention_mask\"])\n","    output_vectors.append(lex_weights)"]}],"metadata":{"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":2}
