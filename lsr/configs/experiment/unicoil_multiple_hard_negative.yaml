# @package _global_
defaults:
  - override /dataset@train_dataset: coil_hn_pretokenized
  - override /loss: multiple_negative_loss 
  - override /model: unicoil 
data_collator:
  _target_: lsr.datasets.data_collator.PretokenizedGroupCollator
trainer: 
  data_type: multi-negative
training_arguments:
  fp16: True 
  per_device_train_batch_size: 8 
  learning_rate: 5e-6
  warmup_steps: 0
  max_steps: -1
  save_steps: 4000
  warmup_ratio: 0.1
  num_train_epochs: 5
  dataloader_num_workers: 16 
  dataloader_drop_last: True
tokenizer:
  tokenizer_name: distilbert-base-uncased
  is_fast: False 
model:
  query_encoder:
    model_name_or_dir: distilbert-base-uncased