from transformers import AutoModel, AutoTokenizer
import os

# flan_t5_large
# path = "/ivi/ilps/personal/tnguyen5/jf/lsr_eval/outputs/zero_shot_flan_t5_large/model"
# model_name = "google/flan-t5-large"

# model_path = path + "/shared_encoder"
# tokenizer_path = path + "/tokenizer"
# model = AutoModel.from_pretrained(model_name)
# tokenizer = AutoTokenizer.from_pretrained(model_name)
# if not os.path.exists(model_path):
#     os.makedirs(model_path)
#     model.save_pretrained(model_path)
# if not os.path.exists(tokenizer_path):
#     os.makedirs(tokenizer_path)
#     tokenizer.save_pretrained(tokenizer_path)

# flan_t5_xl
# path = "/ivi/ilps/personal/tnguyen5/jf/lsr_eval/outputs/zero_shot_flan_t5_xl/model"
# model_name = "google/flan-t5-xl"

# model_path = path + "/shared_encoder"
# tokenizer_path = path + "/tokenizer"
# model = AutoModel.from_pretrained(model_name)
# tokenizer = AutoTokenizer.from_pretrained(model_name)
# if not os.path.exists(model_path):
#     os.makedirs(model_path)
#     model.save_pretrained(model_path)
# if not os.path.exists(tokenizer_path):
#     os.makedirs(tokenizer_path)
#     tokenizer.save_pretrained(tokenizer_path)

# flan_t5_xxl
# path = "/ivi/ilps/personal/tnguyen5/jf/lsr_eval/outputs/zero_shot_flan_t5_xxl/model"
# model_name = "google/flan-t5-xxl"

# model_path = path + "/shared_encoder"
# tokenizer_path = path + "/tokenizer"
# model = AutoModel.from_pretrained(model_name)
# tokenizer = AutoTokenizer.from_pretrained(model_name)
# if not os.path.exists(model_path):
#     os.makedirs(model_path)
#     model.save_pretrained(model_path)
# if not os.path.exists(tokenizer_path):
#     os.makedirs(tokenizer_path)
#     tokenizer.save_pretrained(tokenizer_path)


# flan_t5_ul2
# path = "/ivi/ilps/personal/tnguyen5/jf/lsr_eval/outputs/zero_shot_flan_t5_ul2/model"
# model_name = "google/flan-ul2"

# model_path = path + "/shared_encoder"
# tokenizer_path = path + "/tokenizer"
# model = AutoModel.from_pretrained(model_name)
# tokenizer = AutoTokenizer.from_pretrained(model_name)
# if not os.path.exists(model_path):
#     os.makedirs(model_path)
#     model.save_pretrained(model_path)
# if not os.path.exists(tokenizer_path):
#     os.makedirs(tokenizer_path)
#     tokenizer.save_pretrained(tokenizer_path)

# opt2.7b
# path = "/ivi/ilps/personal/tnguyen5/jf/lsr_eval/outputs/zero_shot_opt_2.7b/model"
# model_name = "facebook/opt-2.7b"

# model_path = path + "/shared_encoder"
# tokenizer_path = path + "/tokenizer"
# model = AutoModel.from_pretrained(model_name)
# tokenizer = AutoTokenizer.from_pretrained(model_name)
# if not os.path.exists(model_path):
#     os.makedirs(model_path)
#     model.save_pretrained(model_path)
# if not os.path.exists(tokenizer_path):
#     os.makedirs(tokenizer_path)
#     tokenizer.save_pretrained(tokenizer_path)